{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieMind - Model Training & Evaluation\n",
    "\n",
    "This notebook trains and evaluates ML models for:\n",
    "1. Sentiment Classification (positive/neutral/negative)\n",
    "2. Score Prediction (0-10 scale)\n",
    "3. Clustering Analysis\n",
    "4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.utils.db_manager import DatabaseManager\n",
    "from src.preprocessing.text_processor import TextProcessor\n",
    "from src.models.sentiment_classifier import SentimentClassifier\n",
    "from src.models.score_predictor import ScorePredictor\n",
    "from src.models.clustering import MovieClusterer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews with movie metadata\n",
    "with DatabaseManager() as db:\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        r.review_id,\n",
    "        r.content,\n",
    "        r.rating,\n",
    "        r.text_length,\n",
    "        r.word_count,\n",
    "        m.title,\n",
    "        m.genres,\n",
    "        m.vote_average,\n",
    "        m.runtime\n",
    "    FROM reviews r\n",
    "    JOIN movies m ON r.movie_id = m.movie_id\n",
    "    WHERE r.content IS NOT NULL\n",
    "    LIMIT 5000\n",
    "    \"\"\"\n",
    "    \n",
    "    reviews = db.execute_query(query)\n",
    "\n",
    "df = pd.DataFrame(reviews)\n",
    "print(f\"Loaded {len(df)} reviews\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text processor\n",
    "processor = TextProcessor()\n",
    "\n",
    "# Clean reviews\n",
    "print(\"Preprocessing text...\")\n",
    "df['cleaned_content'] = df['content'].apply(\n",
    "    lambda x: processor.clean_text(x) if pd.notna(x) else \"\"\n",
    ")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n=== Original Review ===\")\n",
    "print(df['content'].iloc[0][:500])\n",
    "\n",
    "print(\"\\n=== Cleaned Review ===\")\n",
    "print(df['cleaned_content'].iloc[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable\n",
    "df['score'] = df['rating'].fillna(df['vote_average'])\n",
    "df_labeled = df[df['score'].notna()].copy()\n",
    "\n",
    "print(f\"Reviews with scores: {len(df_labeled)}\")\n",
    "print(f\"\\nScore distribution:\")\n",
    "print(df_labeled['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment labels\n",
    "classifier = SentimentClassifier(model_type='logistic', max_features=5000)\n",
    "\n",
    "df_labeled['sentiment'] = classifier.prepare_sentiment_labels(\n",
    "    df_labeled['score'],\n",
    "    threshold_pos=7.0,\n",
    "    threshold_neg=5.0\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "print(\"Sentiment distribution:\")\n",
    "print(df_labeled['sentiment'].value_counts())\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_labeled['sentiment'].value_counts().plot(kind='bar')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_labeled['cleaned_content'],\n",
    "    df_labeled['sentiment'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_labeled['sentiment']\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "print(\"Training sentiment classifier...\")\n",
    "train_metrics = classifier.train(X_train, y_train, validation_split=0.2)\n",
    "\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in train_metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = classifier.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=test_metrics['confusion_matrix'],\n",
    "    display_labels=classifier.model.classes_\n",
    ")\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix - Sentiment Classification')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features per class\n",
    "importance = classifier.get_feature_importance(top_n=15)\n",
    "\n",
    "print(\"\\n=== Top Features Per Sentiment ===\")\n",
    "for class_name, features in importance.items():\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    for feat, score in features[:10]:\n",
    "        print(f\"  {feat:20s}: {score:7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Score Prediction (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata features\n",
    "meta_features = ['text_length', 'word_count']\n",
    "df_labeled[meta_features] = df_labeled[meta_features].fillna(0)\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, X_meta_train, X_meta_test, y_score_train, y_score_test = train_test_split(\n",
    "    df_labeled['cleaned_content'],\n",
    "    df_labeled[meta_features],\n",
    "    df_labeled['score'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_text_train)}\")\n",
    "print(f\"Test size: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train score predictor\n",
    "predictor = ScorePredictor(model_type='ridge', max_features=3000)\n",
    "\n",
    "print(\"Training score predictor...\")\n",
    "train_metrics = predictor.train(\n",
    "    X_text_train,\n",
    "    y_score_train,\n",
    "    X_meta_train,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in train_metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_metrics = predictor.evaluate(X_text_test, y_score_test, X_meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(test_metrics['predictions'], test_metrics['residuals'], alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Score')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[1].scatter(y_score_test, test_metrics['predictions'], alpha=0.5)\n",
    "axes[1].plot([0, 10], [0, 10], 'r--', label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual Score')\n",
    "axes[1].set_ylabel('Predicted Score')\n",
    "axes[1].set_title('Predicted vs Actual')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features for regression\n",
    "importance_reg = predictor.get_feature_importance(top_n=20)\n",
    "\n",
    "print(\"\\n=== Top Features for Score Prediction ===\")\n",
    "for feat, score in importance_reg[:15]:\n",
    "    print(f\"  {feat:20s}: {score:7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "df_cluster = df_labeled.copy()\n",
    "\n",
    "# Initialize clusterer\n",
    "clusterer = MovieClusterer(n_clusters=5, random_state=42)\n",
    "\n",
    "# Prepare features (text + numeric)\n",
    "features = clusterer.prepare_features(\n",
    "    df_cluster,\n",
    "    text_column='cleaned_content',\n",
    "    numeric_columns=['text_length', 'word_count', 'score']\n",
    ")\n",
    "\n",
    "print(f\"Feature matrix shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow analysis\n",
    "print(\"Performing elbow analysis...\")\n",
    "elbow_results = clusterer.elbow_analysis(features, max_k=10)\n",
    "\n",
    "print(\"\\nElbow Analysis Results:\")\n",
    "display(elbow_results)\n",
    "\n",
    "# Plot\n",
    "clusterer.plot_elbow(elbow_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit clustering with optimal k (adjust based on elbow plot)\n",
    "optimal_k = 5  # Adjust this based on elbow plot\n",
    "clusterer = MovieClusterer(n_clusters=optimal_k, random_state=42)\n",
    "\n",
    "labels, metrics = clusterer.fit_predict(features)\n",
    "\n",
    "print(\"\\nClustering Metrics:\")\n",
    "print(f\"  Silhouette Score: {metrics['silhouette_score']:.4f}\")\n",
    "print(f\"  Davies-Bouldin Score: {metrics['davies_bouldin_score']:.4f}\")\n",
    "print(f\"  Inertia: {metrics['inertia']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "clusterer.visualize_clusters_2d(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster distribution\n",
    "cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "cluster_counts.plot(kind='bar')\n",
    "plt.title('Reviews per Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster summary\n",
    "summary = clusterer.get_cluster_summary(\n",
    "    df_cluster,\n",
    "    labels,\n",
    "    numeric_columns=['text_length', 'word_count', 'score']\n",
    ")\n",
    "\n",
    "print(\"\\n=== Cluster Summary ===\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA: Score differences across clusters\n",
    "cluster_groups = []\n",
    "for i in range(optimal_k):\n",
    "    cluster_scores = df_cluster[labels == i]['score'].dropna()\n",
    "    cluster_groups.append(cluster_scores)\n",
    "\n",
    "f_stat, p_value = stats.f_oneway(*cluster_groups)\n",
    "\n",
    "print(\"\\n=== ANOVA: Score across Clusters ===\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant: {'Yes (p < 0.05)' if p_value < 0.05 else 'No (p >= 0.05)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-squared: Sentiment vs Cluster\n",
    "contingency = pd.crosstab(labels, df_cluster['sentiment'])\n",
    "\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "\n",
    "print(\"\\n=== Chi-squared: Sentiment vs Cluster ===\")\n",
    "print(f\"Chi¬≤ statistic: {chi2:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"Significant: {'Yes (p < 0.05)' if p_value < 0.05 else 'No (p >= 0.05)'}\")\n",
    "\n",
    "print(\"\\nContingency Table:\")\n",
    "display(contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation: Text length vs Score\n",
    "valid_data = df_cluster[['text_length', 'score']].dropna()\n",
    "corr, p_value = stats.pearsonr(valid_data['text_length'], valid_data['score'])\n",
    "\n",
    "print(\"\\n=== Correlation: Text Length vs Score ===\")\n",
    "print(f\"Pearson correlation: {corr:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant: {'Yes (p < 0.05)' if p_value < 0.05 else 'No (p >= 0.05)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "import os\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save sentiment classifier\n",
    "classifier.save_model('../models/sentiment_classifier')\n",
    "print(\"‚úì Sentiment classifier saved\")\n",
    "\n",
    "# Save score predictor\n",
    "predictor.save_model('../models/score_predictor')\n",
    "print(\"‚úì Score predictor saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"  Total reviews: {len(df_labeled)}\")\n",
    "print(f\"  Train samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\nüéØ Sentiment Classification:\")\n",
    "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Score Prediction:\")\n",
    "test_reg_metrics = predictor.evaluate(X_text_test, y_score_test, X_meta_test)\n",
    "print(f\"  R¬≤ Score: {test_reg_metrics['r2']:.4f}\")\n",
    "print(f\"  RMSE: {test_reg_metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE: {test_reg_metrics['mae']:.4f}\")\n",
    "\n",
    "print(f\"\\nüîç Clustering:\")\n",
    "print(f\"  Number of clusters: {optimal_k}\")\n",
    "print(f\"  Silhouette Score: {metrics['silhouette_score']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Models saved to: ../models/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
