# MovieMind - Was noch zu tun ist

**Stand**: 18. November 2025
**Abgabe**: 11. Januar 2026

---

## âœ… BEREITS FERTIG

### Code & Infrastruktur (100% komplett)
- âœ… **Projektstruktur**: Alle Ordner und Module erstellt
- âœ… **PostgreSQL Schema**: Tabellen (`movies`, `reviews`, `countries`), Views, Indexes
- âœ… **API Client**: TMDb-Integration mit Rate-Limiting
- âœ… **Text-Preprocessing**: HTML-Cleanup, StopwÃ¶rter, Lemmatisierung (NLTK)
- âœ… **Sentiment-Classifier**: TF-IDF + LogReg/RF, alle Metriken
- âœ… **Score-Predictor**: Ridge/Lasso Regression mit Meta-Features
- âœ… **Clustering**: K-means mit Elbow, Silhouette
- âœ… **Evaluation-Skript**: Confusion Matrix, Residuenplots, p-Werte
- âœ… **Dashboard**: Flask/Dash Template

### Notebooks (100% komplett)
- âœ… `01_exploratory_analysis.ipynb` - EDA mit ChiÂ², ANOVA, Korrelation
- âœ… `02_model_training.ipynb` - Model Training
- âœ… `03_clustering_analysis.ipynb` - K-means, Elbow, Silhouette
- âœ… `04_geo_visualization.ipynb` - Choropleth-Karten nach Land

### Dokumentation (100% komplett)
- âœ… `README.md` - Projekt-Ãœbersicht
- âœ… `QUICKSTART.md` - Detaillierte Setup-Anleitung
- âœ… `PRESENTATION_OUTLINE.md` - VollstÃ¤ndiger PrÃ¤sentationsleitfaden
- âœ… `requirements.txt` - Alle Dependencies
- âœ… `.env.sample` - Konfigurations-Template
- âœ… `setup_project.py` - Automatisches Setup-Skript

---

## ðŸ”´ WAS DU NOCH MACHEN MUSST

### 1. SETUP & KONFIGURATION (30-60 Minuten)

#### A. API-Keys einrichten
```bash
#gemacht
# 1. TMDb API Key besorgen
# - Gehe zu: https://www.themoviedb.org/settings/api
# - Account erstellen (falls nicht vorhanden)
# - API Key beantragen (Developer Section)
# - Kopiere den API Key (v3 auth)

# 2. .env Datei konfigurieren
cp .env.sample .env
# Editiere .env und fÃ¼ge deinen API Key ein:
TMDB_API_KEY=dein_echter_api_key_hier
```

#### B. PostgreSQL einrichten
```bash
# 1. PostgreSQL installieren (falls noch nicht)
# - Windows: https://www.postgresql.org/download/windows/
# - Installiere mit Standardeinstellungen

# 2. Datenbank erstellen
psql -U postgres
CREATE DATABASE moviemind;
\q

# 3. Schema initialisieren
psql -U postgres -d moviemind -f sql/schema.sql

# ODER automatisch via Setup-Skript:
python setup_project.py
```

#### C. Python-Umgebung
```bash
# Virtual Environment erstellen
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Mac/Linux

# Dependencies installieren
pip install -r requirements.txt

# NLTK Daten herunterladen (automatisch beim ersten Start)
```

---

### 2. DATENSAMMLUNG (2-4 Stunden)

**WICHTIG**: Dies ist der zeitaufwendigste Schritt!

```bash
# Aktiviere Virtual Environment
venv\Scripts\activate

# Sammle 500-1000 Filme + Reviews
python src/data_collection/fetch_movies.py --movies 500 --strategy mixed

# Hinweise:
# - API hat Rate-Limits (ca. 40 Requests/10 Sekunden)
# - Das Skript hat automatische Delays eingebaut
# - FÃ¼r 500 Filme + Reviews: ca. 2-3 Stunden
# - LÃ¤uft im Hintergrund, du kannst wÃ¤hrenddessen anderes machen

# ÃœberprÃ¼fe Datensammlung
psql -U postgres -d moviemind
SELECT COUNT(*) FROM movies;   -- Sollte ~500 sein
SELECT COUNT(*) FROM reviews;  -- Sollte >5000 sein
\q
```

**Strategie-Optionen**:
- `--strategy popular`: Nur populÃ¤re Filme
- `--strategy top_rated`: Nur top-bewertete Filme
- `--strategy mixed`: Mix aus beidem (empfohlen!)

---

### 3. MODELLTRAINING (1-2 Stunden)

```bash
# Nachdem Daten gesammelt sind:

# 1. EDA-Notebook ausfÃ¼hren (optional, aber hilfreich)
jupyter notebook notebooks/01_exploratory_analysis.ipynb
# FÃ¼hre alle Zellen aus, speichere Plots fÃ¼r PrÃ¤sentation

# 2. Modelle trainieren
python src/models/train_models.py

# Das trainiert:
# - Sentiment Classifier (TF-IDF + LogReg)
# - Score Predictor (Ridge Regression)
# Speichert Modelle in models/

# 3. Modelle evaluieren
python src/models/evaluate_models.py

# Erstellt:
# - evaluation_results/confusion_matrix_sentiment.png
# - evaluation_results/regression_evaluation.png
```

---

### 4. ERWEITERTE ANALYSEN (2-3 Stunden)

```bash
# FÃ¼hre alle Notebooks aus (in Reihenfolge):

jupyter notebook

# Ã–ffne und fÃ¼hre aus:
# 1. notebooks/01_exploratory_analysis.ipynb
#    â†’ Speichere wichtige Plots (Rating-Verteilung, Korrelation)
#
# 2. notebooks/03_clustering_analysis.ipynb
#    â†’ Speichere Elbow-Plot, Silhouette-Plot, PCA-Viz
#
# 3. notebooks/04_geo_visualization.ipynb
#    â†’ Speichere Choropleth-Karten (HTML interaktiv)

# Screenshots fÃ¼r PrÃ¤sentation:
# - Speichere alle wichtigen Plots in presentation/screenshots/
```

---

### 5. PRÃ„SENTATION ERSTELLEN (4-6 Stunden)

#### A. Folien erstellen (3-4 Stunden)

Nutze `PRESENTATION_OUTLINE.md` als Vorlage!

**Empfohlene Tools**:
- Google Slides / PowerPoint
- LaTeX Beamer (fÃ¼r akademische Optik)
- Canva (fÃ¼r schÃ¶ne Grafiken)

**Struktur** (siehe PRESENTATION_OUTLINE.md):
1. **Title Slide**: Projekt-Name, Team, Datum
2. **Intro & Motivation**: Problem, LÃ¶sung, Wert
3. **Scope**: Datenquellen, Ziele
4. **Methodology**: API â†’ DB â†’ NLP â†’ ML
5. **EDA**: Plots, ChiÂ², ANOVA, Korrelation
6. **ML Models**: Classifier + Regressor Metriken
7. **Clustering**: Elbow, Silhouette, Interpretation
8. **Geo-Insights**: Choropleth-Karten
9. **Demo**: Dashboard-Screenshot oder Live
10. **Results**: Metriken-Zusammenfassung
11. **Challenges**: API-Limits, Bias, LÃ¶sungen
12. **Conclusions**: Key Achievements, Future Work
13. **Bonus Checklist**: PostgreSQL âœ“, ChiÂ² âœ“, k-means âœ“...
14. **Q&A**

**Wichtige Inhalte**:
- **Screenshots**:
  - Database schema (`psql -d moviemind` â†’ `\dt`)
  - Confusion Matrix
  - Residuenplots
  - Elbow-Plot
  - Choropleth-Karte
  - Dashboard-Demo

- **Metriken** (fÃ¼lle mit echten Werten):
  - Sentiment Accuracy: ___%
  - Score RÂ²: ___
  - Silhouette Score: ___
  - ChiÂ² p-value: ___
  - ANOVA p-value: ___

#### B. Bonus-Anhang erstellen (1 Stunde)

Erstelle `appendix_bonus_points.pdf` mit:

1. **PostgreSQL-Nachweis**:
   - Screenshot `\dt` (Tabellen-Liste)
   - Screenshot SQL-View-Code
   - Screenshot Query-Ergebnisse

2. **Geodaten**:
   - Screenshot Choropleth-Karte (Sentiment by Country)
   - Code-Snippet: plotly.express.choropleth()

3. **Statistische Tests**:
   - ChiÂ²-Test Output (mit **p-value < 0.05** hervorgehoben)
   - ANOVA Output (mit **p-value** und **F-statistic**)
   - Korrelation Output (Pearson r, **p-value**)

4. **K-means**:
   - Elbow-Plot (mit optimalem k markiert)
   - Silhouette-Score-Tabelle
   - Cluster-Interpretation

5. **Klassifikation & Regression**:
   - Confusion Matrix (mit Accuracy %)
   - Precision/Recall/F1 Tabelle
   - RÂ²/RMSE/MAE Tabelle
   - Residuenplot

---

### 6. VIDEO-AUFNAHME (2-3 Stunden)

#### Vorbereitung (30 Min)
- [ ] Folien finalisieren (PDF exportieren)
- [ ] Skript/StichwÃ¶rter fÃ¼r jeden Sprecher vorbereiten
- [ ] Timer bereitstellen (15 Min genau!)
- [ ] Screen-Recording-Software testen (OBS, Zoom, PowerPoint)

#### Probe-Durchlauf (30 Min)
- [ ] Timing checken (3 Personen Ã— 5 Min)
- [ ] ÃœbergÃ¤nge zwischen Sprechern Ã¼ben
- [ ] Dashboard-Demo proben (oder Pre-Record)

#### Finale Aufnahme (1-2 Stunden)
- [ ] Aufnahme starten
- [ ] PrÃ¤sentation durchfÃ¼hren (15 Min exakt)
- [ ] Video exportieren als MP4 (1080p)
- [ ] QualitÃ¤ts-Check (Audio klar? Folien lesbar?)

**Technik-Tipps**:
- Gutes Mikrofon verwenden
- Ruhige Umgebung
- Bildschirm teilen (Folien im Vollbild)
- Wenn Demo-Risiko: Pre-Record Dashboard, fÃ¼ge Screenshot ein

---

### 7. FINALE DELIVERABLES (1 Stunde)

#### Dateien vorbereiten:

```bash
# 1. PDF erstellen
# - Exportiere Folien als: presentation_group_XX.pdf

# 2. Video benennen
# - Benenne Video: video_recording_group_XX.mp4

# 3. Materials ZIP erstellen
# Packe alles in ein ZIP (OHNE data/, models/, venv/):

# Windows:
# - Rechtsklick auf MovieMind-Ordner â†’ Senden an â†’ ZIP
# - Oder nutze 7-Zip

# Beinhalten sollte:
materials_group_XX.zip
  â”œâ”€â”€ src/               # Alle Python-Skripte
  â”œâ”€â”€ notebooks/         # Alle Jupyter Notebooks (.ipynb)
  â”œâ”€â”€ sql/               # schema.sql
  â”œâ”€â”€ dashboards/        # Dashboard-Code
  â”œâ”€â”€ README.md
  â”œâ”€â”€ QUICKSTART.md
  â”œâ”€â”€ requirements.txt
  â”œâ”€â”€ .env.sample
  â”œâ”€â”€ setup_project.py
  â””â”€â”€ appendix_bonus_points.pdf

# NICHT einpacken:
# - data/ (zu groÃŸ)
# - models/ (zu groÃŸ)
# - venv/ (nicht nÃ¶tig)
# - .git/ (nicht nÃ¶tig)
# - __pycache__/ (automatisch generiert)
```

#### Upload zu Moodle:

- [ ] `presentation_group_XX.pdf`
- [ ] `video_recording_group_XX.mp4`
- [ ] `materials_group_XX.zip`

**Deadline**: Sonntag, 11. Januar 2026

---

## ðŸ“… ZEITPLAN (Empfehlung)

### Woche 1 (18.-24. Nov): Setup & Datensammlung
- **Tag 1-2**: API-Keys, PostgreSQL, Setup (2 Std)
- **Tag 3-5**: Datensammlung laufen lassen (Hintergrund, 3 Std Arbeit)
- **Tag 6-7**: EDA-Notebook ausfÃ¼hren, erste Plots (2 Std)

### Woche 2-4 (25. Nov - 15. Dez): Modelle & Analysen
- **Woche 2**: Modelltraining, Evaluation (4 Std)
- **Woche 3**: Clustering, Geo-Viz Notebooks (4 Std)
- **Woche 4**: Dashboard testen, Screenshots sammeln (2 Std)

### Woche 5-7 (16. Dez - 05. Jan): PrÃ¤sentation
- **16.-22. Dez**: Folien erstellen (6 Std)
- **23.-29. Dez**: Bonus-Anhang, Metriken einfÃ¼gen (3 Std)
- **30. Dez - 05. Jan**: Probe-DurchlÃ¤ufe, Video-Aufnahme (4 Std)

### Woche 8 (06.-11. Jan): Finalisierung
- **06.-08. Jan**: Letzte Korrekturen, QualitÃ¤ts-Check
- **09.-10. Jan**: ZIP erstellen, Moodle-Upload vorbereiten
- **11. Jan**: UPLOAD (vor Deadline!)

---

## ðŸŽ¯ PRIORITÃ„TEN

### MUST HAVE (kritisch fÃ¼r Bestehen):
1. âœ… Datensammlung (500+ Filme, 5000+ Reviews)
2. âœ… PostgreSQL-Schema funktioniert
3. âœ… Sentiment-Classifier trainiert (Accuracy >80%)
4. âœ… Score-Predictor trainiert (RÂ² >0.5)
5. âœ… EDA mit statistischen Tests (ChiÂ², ANOVA, Korrelation)
6. âœ… PrÃ¤sentation (15 Min, PDF + Video)
7. âœ… Materials ZIP (vollstÃ¤ndig, lauffÃ¤hig)

### NICE TO HAVE (Bonus-Punkte):
- âœ… K-means Clustering
- âœ… Geo-Visualisierung (Choropleth)
- âœ… Interaktives Dashboard
- âœ… Gut dokumentierter Code
- âœ… README mit Reproduzierbarkeit

---

## ðŸ†˜ TROUBLESHOOTING

### Problem: API-Key funktioniert nicht
**LÃ¶sung**:
```bash
# Test API-Key:
python src/data_collection/tmdb_client.py
# Sollte "Found X popular movies" zeigen
```

### Problem: PostgreSQL Verbindungsfehler
**LÃ¶sung**:
```bash
# Starte PostgreSQL (Windows):
net start postgresql-x64-14

# ÃœberprÃ¼fe .env Credentials:
DB_HOST=localhost
DB_USER=postgres
DB_PASSWORD=dein_postgres_passwort
```

### Problem: "No module named 'sklearn'"
**LÃ¶sung**:
```bash
venv\Scripts\activate
pip install -r requirements.txt
```

### Problem: NLTK Daten fehlen
**LÃ¶sung**:
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

---

## ðŸ“ž NÃ„CHSTE SCHRITTE (SOFORT)

1. **JETZT MACHEN** (10 Min):
   ```bash
   # Setup ausfÃ¼hren
   python setup_project.py
   ```

2. **HEUTE** (30 Min):
   - TMDb API Key besorgen
   - PostgreSQL installieren (falls nÃ¶tig)
   - .env konfigurieren

3. **DIESE WOCHE** (3 Std):
   - Datensammlung starten und laufen lassen
   - EDA-Notebook durchgehen

4. **NÃ„CHSTE WOCHE**:
   - Modelle trainieren
   - Erste PrÃ¤sentations-Skizze

---

## âœ… CHECKLISTE VOR ABGABE

### Code
- [ ] Alle Notebooks ausgefÃ¼hrt, Outputs gespeichert
- [ ] Models trainiert und in `models/` gespeichert
- [ ] Evaluation-Plots in `evaluation_results/`
- [ ] Dashboard getestet (Screenshots gemacht)

### Dokumentation
- [ ] README.md aktuell und korrekt
- [ ] requirements.txt vollstÃ¤ndig
- [ ] .env.sample vorhanden (OHNE echte Keys!)

### PrÃ¤sentation
- [ ] PDF-Folien (14+ Slides)
- [ ] Alle Metriken eingefÃ¼gt (echte Werte!)
- [ ] Screenshots eingebunden
- [ ] Bonus-Anhang erstellt

### Video
- [ ] 15 Minuten exakt
- [ ] Gute Audio-QualitÃ¤t
- [ ] Folien lesbar
- [ ] MP4-Format, <500 MB

### Upload
- [ ] Dateinamen korrekt: `presentation_group_XX.pdf`
- [ ] Video: `video_recording_group_XX.mp4`
- [ ] ZIP: `materials_group_XX.zip`
- [ ] Moodle-Upload VOR Deadline

---

**VIEL ERFOLG! ðŸŽ¬ðŸ“ŠðŸš€**

Bei Fragen: Schau in `QUICKSTART.md` oder `README.md`!
